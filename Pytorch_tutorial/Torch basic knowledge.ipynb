{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is tensor([[2., 3.],\n",
      "        [4., 8.],\n",
      "        [7., 9.]])\n",
      "a size is torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[2,3],[4,8],[7,9]])\n",
    "print('a is {}'.format(a))\n",
    "print('a size is {}'.format(a.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is tensor([[2, 3],\n",
      "        [4, 8],\n",
      "        [7, 9]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.LongTensor([[2,3],[4,8],[7,9]])\n",
    "print('b is {}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero tensor is tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.zeros((3,2))\n",
    "print('zero tensor is {}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero tensor is tensor([[-1.8671,  0.0204],\n",
      "        [ 0.6895,  1.7295],\n",
      "        [ 0.1434, -0.3827]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.randn((3,2))\n",
    "print('zero tensor is {}'.format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可以像numpy一样，通过索引取得其中的元素，改变它的值，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed a is tensor([[  2.,   3.],\n",
      "        [  4.,   8.],\n",
      "        [100.,   9.]])\n"
     ]
    }
   ],
   "source": [
    "a[2,0] = 100\n",
    "print('changed a is {}'.format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Tensor 与 numpy.ndarray 之间相互转换\n",
    "a.numpy() ---convert to numpy, __a:tensor__    \n",
    "torch.__from_numpy(a) --__a:numpy array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conver to numpy is [[2 3]\n",
      " [4 8]\n",
      " [7 9]]\n"
     ]
    }
   ],
   "source": [
    "numpyB = b.numpy()\n",
    "print('conver to numpy is {}'.format(numpyB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conver to tensor is tensor([[2, 3],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "e = np.array([[2,3],[3,4]])\n",
    "tensorE = torch.from_numpy(e)\n",
    "print('conver to tensor is {}'.format(tensorE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change the type of tensor to float, like a.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float torch is tensor([[2., 3.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "ftorchE = tensorE.float()\n",
    "print('float torch is {}'.format(ftorchE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过torch.cuda.is_available()判断，电脑是否支持GPU。\n",
    "\n",
    "支持的话，如果想__把tensor a放到GPU上__，只需a.cuda()就可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.,   3.],\n",
      "        [  4.,   8.],\n",
      "        [100.,   9.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    a_cuda = a.cuda()\n",
    "    print(a_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 Variable  \n",
    "Variable 在numpy中不存在，提供了自动求导的功能。   \n",
    "TensorFlow，神经网络在做运算时，需要先构造一个计算图谱，然后，在里面进行前向传播和反向传播。  \n",
    "\n",
    "Variable会被放入一个计算图中，然后进行前向传播，反向传播，自动求导。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [4., 8.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor and Variable 没有本质区别，\n",
    "from torch.autograd import Variable\n",
    "a = torch.Tensor([[2,3],[4,8]])\n",
    "va = Variable(a)\n",
    "va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable 有三个属性：\n",
    "\n",
    "1. data：取出Variable里面Tensor的值\n",
    "2. grad：保存了Variable的反向传播梯度\n",
    "3. grad_fn：得到Variable的操作\n",
    "\n",
    "__requires_grad=True，表示对该变量求梯度__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. scalar gradient：标量求导，backward函数的参数可以不写。     \n",
    "y.backward() is equal to y.backward(torch.FloatTensor([1]))  \n",
    "\n",
    "## 2. 对矩阵求导，backward函数必须有参数。   \n",
    "y.backward(torch.FloatTensor([1, 0.1, 0.01]))   \n",
    "得到的结果就是他们每个分量的梯度---得到的梯度就是他们原本的梯度分别 *乘以 1， 0.1， 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "y = x * 2\n",
    "#y.backward()\n",
    "#print(x.grad)\n",
    "y.backward(torch.FloatTensor([1, 0.1, 0.01]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0519,  1.8997, -1.8134], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.3 Dataset   \n",
    "__torch.utils.data.Dataset__ is abstract class(抽象类).  \n",
    "定义你的数据类可以 继承和重写这个抽象类，只需要定义 \\_\\_len\\_\\_ and \\_\\_getitem__ two functions.  \n",
    "通过迭代的方式来取得每一个数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, csv_file, txt_file, root_dir, other_file):\n",
    "        self.csv_data = pd.read_csv(csv_file)\n",
    "        with open(txt_file, 'r') as f:\n",
    "            data_list = f.readlines()\n",
    "        self.txt_data = data_list\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.csv_data[idx], self.txt_data[idx])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__torch.utils.data.DataLoader__ 定义一个新的迭代器   \n",
    "可以取batch,shuffle，或者多线程去读取数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class example(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([8, 9])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = list(range(10))\n",
    "mydata = example(data)\n",
    "loader = DataLoader(mydata, batch_size=4)  # each row, we have batch-size is 4\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.4 nn.Module\n",
    "\n",
    "所有层结构和损失函数都来自于torch.nn。\n",
    "__层结构__: 线性层nn.Linear, 1维convolutional layer，2d convolutional layer  \n",
    "__损失函数__: nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.5 torch.optim\n",
    "\n",
    "torch.optim.SGD：随机梯度下降（stochastic gradient descent）\n",
    "torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.6 Save and load the model\n",
    "\n",
    "1. save all constructures and parameters of the whole model. Therefore, save model.  \n",
    "torch.save(model, './model.path')  \n",
    "torch.load('./model.path')  \n",
    "\n",
    "2. save parameters of the model. Therefore, save the state of the model.  \n",
    "torch.save(model.state_dict(0, './model_state.path')  \n",
    "state = torch.load('model_state.path')  \n",
    "model.load_state_dic(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
