{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from ift725.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'ift725/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ift725.classifiers.softmax import softmax_naive_loss_function\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_dev\n",
    "y = y_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.0\n",
    "dW = np.zeros_like(W)\n",
    "loss = loss*0\n",
    "dW = dW*0\n",
    "num_train = X.shape[0]\n",
    "num_class = W.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18104247,  0.        , -0.85221824, ..., -0.10439289,\n",
       "        -0.6412275 , -0.8101488 ],\n",
       "       [-0.63658807, -0.35433154,  0.        , ..., -0.33561831,\n",
       "        -0.37079301, -0.42414129],\n",
       "       [-0.68235898, -0.60299003, -0.85404082, ..., -0.18372438,\n",
       "        -0.02639542, -0.57437789],\n",
       "       ...,\n",
       "       [-0.7324268 , -0.74505883, -0.29137426, ..., -0.7874497 ,\n",
       "        -0.60610546,  0.        ],\n",
       "       [-0.707259  , -1.83676208, -0.53651233, ..., -1.162221  ,\n",
       "        -0.93154186,  0.        ],\n",
       "       [-0.38328642, -0.16830871, -0.27042709, ..., -0.0149319 ,\n",
       "         0.        , -0.8712581 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = X.dot(W)\n",
    "f = scores - np.max(scores, axis=1, keepdims=True)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma = np.sum(np.exp(f), axis=1)\n",
    "suma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = np.exp(f)/np.exp(f).sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1205019 ,  0.14441743,  0.0615894 , ...,  0.13010152,\n",
       "         0.0760568 ,  0.0642357 ],\n",
       "       [ 0.0759809 ,  0.1007597 ,  0.14360551, ...,  0.10266299,\n",
       "        -0.90088537,  0.0939656 ],\n",
       "       [ 0.07742282,  0.08381823,  0.06520914, ...,  0.12747447,\n",
       "        -0.85080636,  0.08625108],\n",
       "       ...,\n",
       "       [ 0.07483243,  0.07389309,  0.11631522, ...,  0.07082616,\n",
       "        -0.91509163,  0.15566063],\n",
       "       [ 0.10966505, -0.96455693,  0.13008361, ...,  0.06957941,\n",
       "         0.08763214,  0.22244718],\n",
       "       [ 0.1051128 ,  0.13032245,  0.11767109, ...,  0.15192524,\n",
       "         0.1542108 ,  0.06452562]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = np.sum(-np.log(probability[range(num_train), y]))\n",
    "probability[range(num_train), y] -= 1\n",
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.23932229e+03, -4.73376822e+02, -2.89847538e+01, ...,\n",
       "        -3.25967613e+02, -8.48569558e+02, -1.56251204e+03],\n",
       "       [-1.89332930e+03, -7.42702137e+01, -2.12503295e+02, ...,\n",
       "        -4.15074029e+02, -1.40835042e+03, -1.65596326e+03],\n",
       "       [-3.19773672e+03, -2.89085761e+01, -1.13885033e+01, ...,\n",
       "        -5.89641607e+02, -2.32529070e+03, -1.93775167e+03],\n",
       "       ...,\n",
       "       [-1.42685204e+03,  5.76930100e+01, -2.63813149e+02, ...,\n",
       "         3.28336801e+02,  7.12363273e+02, -2.79464728e+02],\n",
       "       [-2.34049750e+03, -5.70825336e+01, -2.50880078e+02, ...,\n",
       "         1.02041285e+03,  1.82898658e+02, -3.59818672e+02],\n",
       "       [ 2.39635273e+00,  3.79009599e+00,  3.83538202e+00, ...,\n",
       "        -6.07449985e+00,  1.01439689e+01,  8.77245897e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW = X.T.dot(probability)\n",
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
